
# Student Assignment: End-to-End Machine Learning Project

## Objective
In this assignment, you will work with a **realistic synthetic dataset** of 10,000 rows and 20 features, designed to mimic challenges in real-world data science projects. 
Your goal is to perform **complete data preprocessing, exploratory data analysis (EDA), feature engineering, and machine learning modeling**, and finally compare the performance of different algorithms.

---

## Dataset Details
- **File Provided:** synthetic_dataset_10000x20.csv
- **Shape:** 10,000 rows × 21 columns (20 features + 1 target)
- **Target Column:** `target_default_risk` (binary: 0 = No Default, 1 = Default)
- **Features include:**
  - Numeric (e.g., `age`, `income`, `credit_score`, `loan_amount`)
  - Categorical (e.g., `education`, `marital_status`, `region`)
  - Dates (e.g., `signup_date`)
  - Engineered features (e.g., `debt_to_income`, `sin_age`)
  - Some missing values, typos in categories, and outliers (to simulate real-world messiness).

---

## Tasks

### 1. Exploratory Data Analysis (EDA)
- Check dataset shape, data types, missing values, and unique values.
- Plot distributions of numeric features (histograms, boxplots).
- Analyze categorical variables (bar charts, value counts).
- Detect outliers and discuss strategies to handle them.
- Explore correlations (heatmap, pairplot).
- Look at class balance in `target_default_risk`.

### 2. Data Preprocessing
- Handle missing values (imputation strategies).
- Fix categorical inconsistencies (e.g., typos in `education` like "Bachlors").
- Encode categorical variables (one-hot encoding / ordinal encoding).
- Scale numeric features (StandardScaler/MinMaxScaler for algorithms like Logistic Regression, SVM).
- Create or transform features if useful (e.g., income per dependent, recency from `signup_date`).
- Outlier treatment (winsorization, capping, or removal).

### 3. Model Building
Train and evaluate the following models:
1. **Logistic Regression**
2. **Decision Tree**
3. **Support Vector Machine (SVM)**
4. **Random Forest**
5. **XGBoost**

- Use train/test split or cross-validation.
- Evaluate using **accuracy, precision, recall, F1-score, confusion matrix**.

### 4. Hyperparameter Tuning
- Use GridSearchCV or RandomizedSearchCV to tune models (especially Random Forest and XGBoost).
- Compare results with baseline models.

---

## Target Performance (Approximate Benchmarks)
Your models should achieve results close to the following (after proper preprocessing and tuning):
- Logistic Regression → ~90% accuracy
- Decision Tree → ~92%
- SVM → ~95%
- Random Forest → ~96%
- XGBoost → ~98% (can approach ~100% with proper hyperparameter tuning)

*(These numbers are approximate and will vary depending on preprocessing and tuning. The goal is to learn the **process**, not just the final score.)*

---

## Deliverables
You must submit:
1. A **Jupyter Notebook** with:
   - EDA and visualizations
   - Data preprocessing steps
   - Model training, evaluation, and tuning
   - Observations and conclusions
2. A **Short Report** (1–2 pages) summarizing:
   - Key findings from EDA
   - Preprocessing decisions
   - Model comparison and results
   - Final conclusions and reflections

---

## Grading Rubric (100 points)
- **EDA & Preprocessing (30 points)**  
  - Completeness of EDA (10)  
  - Correct handling of missing values, outliers, encoding, scaling (20)

- **Modeling (30 points)**  
  - Proper implementation of 5 models (15)  
  - Correct evaluation metrics (15)

- **Hyperparameter Tuning (20 points)**  
  - Attempt and explanation of tuning (10)  
  - Demonstrated performance improvement (10)

- **Report (20 points)**  
  - Clarity and depth of analysis (10)  
  - Final comparison and conclusions (10)

---

## Reminder
This is **not just about achieving accuracy**. You will be graded on your **process, analysis, and explanation**. A clean, well-documented notebook and a thoughtful report will score higher than just high accuracy numbers.

